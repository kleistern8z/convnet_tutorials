{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4748d-9796-4331-b665-93e6763bd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "import imageio\n",
    "import numpy\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe4293-e0f6-446f-8865-6fcb759b363b",
   "metadata": {},
   "source": [
    "# Tutorial 1c. Convolution\n",
    "\n",
    "The spatial dimensions of the ouput image (width and height) depend on the spatial dimensions of the input image, kernel_size, padding, and striding. In order to build efficient convolutional networks, it's important to understand what the sizes are after after each convolutional layer.\n",
    "\n",
    "In this exersise you will derive the dependency between input and output image sizes. For the sake of simplicity we assume that the input tensor is _square_, i.e., width = height = image_size.\n",
    "\n",
    "We will use the nn.Conv2d layer here. We have not yet discussed what a convolutional layer is yet, but if you set the first two parameters (input channels and output channels) to 1, then this defines a basic convolution.\n",
    "\n",
    "If your code is correct, you should see 'OK'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2cca2-182a-4386-8f28-98569ccec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_output_size(image_size, kernel_size, padding, stride):\n",
    "    ###########################################################################\n",
    "    # Add code that computes the size of the image after a conv layer.        #\n",
    "    ###########################################################################\n",
    "\n",
    "    return output_size\n",
    "\n",
    "\n",
    "# Compare the size of the output of nn.Conv2d with compute_convnet_output_size.\n",
    "for image_size in range(5, 21, 1):\n",
    "    # Shape: batch x channels x height x width.\n",
    "    input_tensor = torch.zeros((1, 1, image_size, image_size))\n",
    "    \n",
    "    for kernel_size in 2, 3, 5, 7:\n",
    "        for padding in 0, 5:\n",
    "            for stride in 1, 2, 3, 4:\n",
    "                if kernel_size >= image_size:\n",
    "                    continue\n",
    "                \n",
    "                output_tensor = Conv2d(1, 1, kernel_size, stride, padding)(input_tensor)\n",
    "                \n",
    "                output_size = output_tensor.size(2)\n",
    "                \n",
    "                predicted_output_size = compute_conv_output_size(image_size, kernel_size, padding, stride)\n",
    "                \n",
    "                assert output_size == predicted_output_size, (\n",
    "                    f\"ERROR: the real size is {output_size},\"\n",
    "                    f\" but got {predicted_output_size}.\"\n",
    "                    f\"\\nimage_size={image_size}\"\n",
    "                    f\" kernel_size={kernel_size}\"\n",
    "                    f\" padding={padding}\"\n",
    "                    f\" stride={stride}\"\n",
    "                )\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af786813-5f81-4154-a5bb-f531f11db00e",
   "metadata": {},
   "source": [
    "You can now use the function you just implemented to compute the size of the output of a convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4cb2fb-560e-4bae-9e1f-bc4b805ba3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_conv_output_size(1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb21a7-fc42-48f8-9ddf-3e47fdb499d6",
   "metadata": {},
   "source": [
    "**Question [optional]:** Implement your own convolution operator **without** using any of PyTorch's (or numpy's) pre-defined convolutional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c11526-2778-48a5-a7cf-23a3d84f28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive Python implementation of a convolution.\n",
    "    The input consists of an image tensor with height H and\n",
    "    width W. We convolve each input with a filter F, where the filter\n",
    "    has height HH and width WW.\n",
    "    Input:\n",
    "    - x: Input data of shape (H, W)\n",
    "    - w: Filter weights of shape (HH, WW)\n",
    "    - b: Bias for filter\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
    "    along the height and width axes of the input. Be careful not to modfiy the original\n",
    "    input x directly.\n",
    "    Returns an array.\n",
    "    - out: Output data, of shape (H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    H, W = x.shape\n",
    "    filter_height, filter_width = w.shape\n",
    "    stride, pad = conv_param[\"stride\"], conv_param[\"pad\"]\n",
    "\n",
    "    # Check dimensions.\n",
    "    assert (W + 2 * pad - filter_width) % stride == 0, \"width does not work\"\n",
    "    assert (H + 2 * pad - filter_height) % stride == 0, \"height does not work\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function torch.nn.functional.pad for padding.     #\n",
    "    ###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c1544-2d7d-4cca-9244-7241b424de82",
   "metadata": {},
   "source": [
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc9ae3-deec-4316-aafd-7fe67415dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make convolution module.\n",
    "w_shape = (4, 4)\n",
    "w = torch.linspace(-0.2, 0.3, steps=torch.prod(torch.tensor(w_shape))).reshape(w_shape)\n",
    "b = torch.linspace(-0.1, 0.2, steps=1)\n",
    "\n",
    "# Compute output of module and compare against reference values.\n",
    "x_shape = (4, 4)\n",
    "x = torch.linspace(-0.1, 0.5, steps=torch.prod(torch.tensor(x_shape))).reshape(x_shape)\n",
    "out = conv_naive(x, w, b, {\"stride\": 2, \"pad\": 1})\n",
    "\n",
    "correct_out = torch.tensor([[0.156, 0.162], [0.036, -0.054]])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print(\"Testing conv_forward_naive\")\n",
    "rel_error = ((out - correct_out) / (out + correct_out + 1e-6)).mean()\n",
    "print(\"difference: \", rel_error)\n",
    "if abs(rel_error) < 1e-6:\n",
    "    print(\"Nice work! Your implementation of a convolution layer works correctly.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Something is wrong. The output was expected to be {} but it was {}\".format(\n",
    "            correct_out, out\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570b9ff-9030-4682-90cd-5ccdb8343a6d",
   "metadata": {},
   "source": [
    "**Aside: Image processing via convolutions:**\n",
    "\n",
    "As fun way to gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (grayscale conversion and edge detection). The convolution forward pass will apply these operations to each of the input images. We can then visualize the results as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a0e4f-a87d-4f98-973d-45f37094123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image of a kitten and a puppy.\n",
    "kitten_uri = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Persian_Cat_%28kitten%29.jpg/256px-Persian_Cat_%28kitten%29.jpg\"\n",
    "puppy_uri = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Golde33443.jpg/256px-Golde33443.jpg\"\n",
    "kitten, puppy = imageio.imread(kitten_uri), imageio.imread(puppy_uri)\n",
    "\n",
    "img_size = 200  # Make this smaller if it runs too slow\n",
    "x = numpy.zeros((2, 3, img_size, img_size))\n",
    "x[0, :, :, :] = skimage.transform.resize(puppy, (img_size, img_size)).transpose(\n",
    "    (2, 0, 1)\n",
    ")\n",
    "x[1, :, :, :] = skimage.transform.resize(kitten, (img_size, img_size)).transpose(\n",
    "    (2, 0, 1)\n",
    ")\n",
    "x = torch.FloatTensor(x)\n",
    "\n",
    "# Set up a convolutional weights holding 2 filters, each 3x3\n",
    "w = torch.zeros((2, 3, 3, 3), dtype=torch.float)\n",
    "\n",
    "# The first filter converts the image to grayscale.\n",
    "# Set up the red, green, and blue channels of the filter.\n",
    "w[0, 0, :, :] = torch.tensor([[0, 0, 0], [0, 0.3, 0], [0, 0, 0]])\n",
    "w[0, 1, :, :] = torch.tensor([[0, 0, 0], [0, 0.6, 0], [0, 0, 0]])\n",
    "w[0, 2, :, :] = torch.tensor([[0, 0, 0], [0, 0.1, 0], [0, 0, 0]])\n",
    "\n",
    "# Second filter detects horizontal edges in the blue channel.\n",
    "w[1, 2, :, :] = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "\n",
    "# Vector of biases. We don't need any bias for the grayscale\n",
    "# filter, but for the edge detection filter we want to add 128\n",
    "# to each output so that nothing is negative.\n",
    "b = torch.tensor([0, 128], dtype=torch.float)\n",
    "\n",
    "# Compute the result of convolving each input in x with each filter in w,\n",
    "# offsetting by b, and storing the results in out.\n",
    "out = F.conv2d(x, w, b, stride=1, padding=1).numpy()\n",
    "\n",
    "\n",
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\"Tiny helper to show images as uint8 and remove axis labels.\"\"\"\n",
    "    if normalize:\n",
    "        img_max, img_min = numpy.max(img), numpy.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    matplotlib.pyplot.imshow(img.astype(\"uint8\"))\n",
    "    matplotlib.pyplot.gca().axis(\"off\")\n",
    "\n",
    "\n",
    "# Show the original images and the results of the conv operation\n",
    "matplotlib.pyplot.subplot(2, 3, 1)\n",
    "imshow_noax(puppy, normalize=False)\n",
    "matplotlib.pyplot.title(\"Original image\")\n",
    "matplotlib.pyplot.subplot(2, 3, 2)\n",
    "imshow_noax(out[0, 0])\n",
    "matplotlib.pyplot.title(\"Grayscale\")\n",
    "matplotlib.pyplot.subplot(2, 3, 3)\n",
    "imshow_noax(out[0, 1])\n",
    "matplotlib.pyplot.title(\"Edges\")\n",
    "matplotlib.pyplot.subplot(2, 3, 4)\n",
    "imshow_noax(kitten, normalize=False)\n",
    "matplotlib.pyplot.subplot(2, 3, 5)\n",
    "imshow_noax(out[1, 0])\n",
    "matplotlib.pyplot.subplot(2, 3, 6)\n",
    "imshow_noax(out[1, 1])\n",
    "matplotlib.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convnet_tutorials",
   "language": "python",
   "name": "convnet_tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
